{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c7b6698",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "Carry over the S3 bucket stored from the first notebook and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f65be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed()\n",
    "\n",
    "%store -r\n",
    "print ('tf_endpoint_name=',tf_endpoint_name)\n",
    "print ('data_bucket_name=',data_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586a1614",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "Define Labels for activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0683b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACT_LABELS = [\"downstairs\",\"upstairs\", \"walking\", \"jogging\", \"standing\", \"sitting\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03828fb",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "Supporting Function to load S3 file into a numpy object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea71c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(bucket_name,key):\n",
    "    s3_client = boto3.client('s3')\n",
    "    obj = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "    \n",
    "    with io.BytesIO(obj[\"Body\"].read()) as f:        \n",
    "        f.seek(0)\n",
    "        data = np.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3faaa09",
   "metadata": {},
   "source": [
    "# Step 4\n",
    "Load S3 files into textX and testY numpy arrays.  textX is the sensor data; testY is the one-hot encoded label for ground truth.  In this notebook, this allows us to compare the predicted class to actual class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = load_data(data_bucket_name,'test/test_data.npy')\n",
    "testy = load_data(data_bucket_name,'test/test_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf946ab9",
   "metadata": {},
   "source": [
    "# Step 6\n",
    "Reshape the data into a multi-dimension array of steps and length of step.  As delivered, the original length is 128 steps and these were trained in 4x32.  This code is editable, if you understand the implictations; it works as delivered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b53f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features, n_outputs = testX.shape[1], testy.shape[1]\n",
    "n_steps, n_length = 4, 32\n",
    "input_data = testX.reshape((testX.shape[0], n_steps, n_length, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac581f",
   "metadata": {},
   "source": [
    "# Step 7\n",
    "Select a random record out of the set, convert the data into JSON and submit to the endpoint for a real-time prediction.  The output will print each class, the probability of being a member of each class and the true class from the testy object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a3d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = random.randint(0,input_data.shape[0])\n",
    "print('random record=',r,)\n",
    "\n",
    "inference_input=input_data[r:r+1]\n",
    "print('inference_input=',inference_input.shape)\n",
    "\n",
    "ep_name = tf_endpoint_name\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "payload=inference_input.tolist()\n",
    "\n",
    "response = client.invoke_endpoint(EndpointName=ep_name,\n",
    "                                   ContentType='application/json',\n",
    "                                   Body=json.dumps(payload)\n",
    "                                 )\n",
    "\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "\n",
    "predictions=result['predictions'][0]\n",
    "\n",
    "for i in range(0,len(predictions)):\n",
    "\n",
    "    print('class=',ACT_LABELS[i], 'proba=',round(float(predictions[i]),4), 'truth=',testy[r][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43063412",
   "metadata": {},
   "source": [
    "# Step 8\n",
    "Optionally, look at the random data row in a tabular fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af78a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = testX[r]\n",
    "df = pd.DataFrame(x[0])\n",
    "\n",
    "for i in range(1,n_features):\n",
    "    df.insert(i, i, x[i], True)\n",
    "    \n",
    "df.columns=(['attitude.roll','attitude.pitch','attitude.yaw','gravity.x','gravity.y','gravity.z','rotationRate.x','rotationRate.y','rotationRate.z','userAcceleration.x','userAcceleration.y','userAcceleration.z'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89eb8a2",
   "metadata": {},
   "source": [
    "# Step 9\n",
    "If you completed step 8, you may visualize the sensor data as time-series movement too across steps.  As delivered, step length is 128, or about 2.5 seconds of activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f792112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(subplots=True,figsize = (20,30))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
